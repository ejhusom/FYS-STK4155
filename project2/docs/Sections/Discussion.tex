\section{Discussion}\label{sec:Discussion}
\subsection{Classification}
\subsubsection{Logistic Regression}
From our results in the case of logistic regression, we see from table \ref{tab:results_classification} that our models performed equally well in terms of accuracy irrespective of the different preprocessing cases, with accuracy scores on the test data in the range $0.80 - 0.82$ in all cases. However, accuracy as a performance metric in a binary classification problem is obviously not a good way to evaluate how well a certain classifier performs, especially when $78\%$ of the outcomes are in the same category. The importance of preprocessing the data correctly becomes more apparent when looking at the rate of true positive and true negative classifications in the same table (\ref{tab:results_classification}). Here we see that, for instance method 1) and 4), where one data set had been balanced in terms of equal outcomes in the training set and the other had not, have nearly identical accuracy scores, but the difference in the models' ability to predict true positives differ by over $10\%$. With only $22\%$ of the observations in the data belonging to the under represented class, a model which trained on this data is easily over fitted to represent the numerous class of outcomes, and our results in these cases are a classical example of this. In our balancing of the data, we have opted to randomly delete cases of the numerous class, however, in a scenario with a smaller data set, it may instead be better to duplicate observations belonging to the under represented class.


\subsubsection{Neural networks}
When using neural networks we started the analysis with tuning the learning parameter and the number of epochs, as we did with logistic regression. The training of the network was very computationally expensive, which prevented us from performing both a cross-validation and a grid search for the optimal parameters. The left plot in figure \ref{fig:eta_epoch_analysis_nn}, indicates that a learning rate of $10^{-1}$ is the most stable choice, even though a higher accuracy score was obtained with preprocessing method 2 and a learning rate of $10^{-2}$. As stated above, the accuracy score is not a very good performance metric, which somewhat undermines the result of these results, but it is nevertheless an indicator of what might work best. In the right plot of figure \ref{fig:eta_epoch_analysis_nn}, we see a general trend of higher accuracy score when increasing the number of epochs for the two data sets that are balanced, while the unbalanced data sets has a decreasing accuracy score. Based on these results, we chose to use the data set with preprocessing method 4 in the next step of our analysis, because it generally had obtained the highest accuracy score. To find the best configuration of number of hidden layers and the amount of nodes in each layer, we performed a grid search, as shown in figure \ref{fig:heatmap_nn_classification}. A problem with this grid search is that we do not necessarily need to have the same amount of nodes in every layer, and we might miss layer configurations which are superior to the ones we tested. Doing a grid search of layer configurations with varying amount of nodes is too costly in terms of computational resources, but we might have obtained better results by doing a random search for more complex layer configurations. Again we face the problem of accuracy score not being the best performance metric, and we can see that the score has only slight variations depending on what configuration of hidden layers that we choose. Even so, the configuration with 3 layers with 80 nodes, which gave the highest test accuracy score, falls into the range of recommended values the acknowledged machine learning textbook by GÃ©ron\cite{geron}[pp. 290-291], and we chose to use that for our final model.



When comparing our results with those of Yeh \& Lien in table \ref{tab:results_classification}, we found that our results for logistic regression in terms of accuracy were very similar, however our classifier achieved a higher area ratio ($0.51$ vs $0.41$). Also with our neural network classifier, out model achieves a higher area ratio than Yeh \& Lien ($0.65$ vs $0.54$). From the same table we can also see that the area ratio on the training data is $0.99$, which may suggest that our neural network is slightly over fitted on the training data. As to why our classifiers perform better in both cases it not easy to say, as Yeh \& Lien do not specify the parameters of their models, nor do they give a description of how they have treated the data prior to training. 





\subsection{Regression}


\subsubsection{Learning rate and number of epochs}

The analysis of the performance of neural networks applied to a regression problem started with a search of the optimal learning rate, as shown in the left plot of figure \ref{fig:eta_epoch_analysis_nn_regression}. We tested both ReLU and sigmoid as activation functions, but as the plot shows, the ReLU was the most stable of the two, and is also the one that is commonly used for regression problems. The results also shows that the lowest MSE were obtained by using a learning rate of either $10^{-1}$ or $10^{-2}$, the former giving slightly better performance. The plot to the right in the same figure shows how much the MSE decreases when we increase the number of epochs, and it indicates that we do not gain much, in terms of MSE, by using more than 500 epochs. When doing this initial analysis, we used two layers with 100, which were chosen on the basis of heuristics in the field of machine learning\cite{geron}.

\subsubsection{Hidden layers configuration}

In order to find the optimal number of layers and number of nodes in each layer, we used a grid search, as shown in figure \ref{fig:heatmap_nn_regression}. In this analysis we used ReLU as activation function with a learning rate of $10^{-1}$, as we previously had found the give the best results, and the heatmap indicates that using 3 layers with 70 nodes in each layer gives the lowest MSE, among the options that we tested. We face the same challenge here as we did when using the neural network for classification, that the optimal configuration might be to have a varying number of nodes in the different layers, but again, this would be a too complex analysis to perform using a grid search. Also, we could try to improve our model by using modified versions of the ReLU function, for example the Leaky ReLU, or even to have different activation functions in different layers. We refrained from exploring these possibilities in order to keep the analysis manageable, but it might have improved the performance of our model.

\subsubsection{The tuned model}

When using the parameters that were found to give the best results, we obtained the model shown in figure \ref{fig:franke20}, where the wiregrid is our neural network prediction, and the coloured surface is the original dataset. The resulting MSE and $R^2$ score is shown in table \ref{tab:regression}, and we can see that the neural network performs significantly worse than the regression methods OLS, Ridge regression and Lasso regression when applied to the same data set. one of the reasons for this might be sub-optimal tuning of the hyperparameters, specifically the learning rate, the number of epochs and the configuration of hidden layers. In our analysis we split the data set into a training set and a set test with a $80\%/20\%$ split, but did not utilize any resampling techniques to optimize the tuning, as we did with logistic regression. As seen in figure \ref{fig:eta_epoch_analysis_nn_regression}, there was little difference between choosing a learning rate of $10^{-1}$ or $10^{-2}$, and the MSE values were obtained on a single training run with a certain random seed. If we had chosen to use the latter learning rate instead of the former, we might have ended up with another configuration of the hidden layers, and possibly better performance metrics. Different seeds also affect the outcome, so by using for example cross-validation we could have tuned our parameters with more certainty.

Another reason for the relatively poor results of our neural network seems to be the small data set size.  As we can see in figure \ref{fig:regression_gridsize}, where the grid size of the Franke function is plotted against the obtained MSE and $R^2$ score of the test set. We can clearly see that we obtain a better performance of our model when we increase the amount of data that is fed to the neural network. We also observe that the test MSE and $R^2$ is also greatly affected by the noise in the data set, and might indicate that the high test MSE of the neural network is a result of overfitting.

The size of the minibatches is another parameter of the model, which we have chosen to have constant through all our analysis. We might have seen improvement by increasing or decreasing the batch size, but because the number of parameters to tune when using a neural network is significant, we chose to leave the batch size at 100, in order to reduce the complexity of the analysis. We could also have introduced so-called regularization\cite{hastie}[p. 398] to our neural network for both regression and classification, but initial testing of this method did not improve our results, so we chose to not include that in our analysis. Furthermore, it is common for stochastic gradient descent implementations to include an adaptive learning rate\footnote{Scikit Learn's documentation on stochastic gradient descent: \href{https://scikit-learn.org/stable/modules/sgd.html}{https://scikit-learn.org/stable/modules/sgd.html}}, which we also tried in the early stages of our code development, but because our implementation of it seemed to give no significant improvement, which might be caused by a bug, we chose to stick with a constant learning rate for the sake of simplicity.
